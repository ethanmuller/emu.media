<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>moufs</title>

    <!-- this prevents a dorky zoomed-out page on mobile -->
    <meta name="viewport" content="width=device-width,initial-scale=1.0">

    <!-- this tells the browser which characters we're using -->
    <meta charset="UTF-8">

    <!-- this gets us nice fonts -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@300&display=swap" rel="stylesheet">


    <style>
      body {
          padding: 0;
          margin: 0;
      }
      .wrapper {
          display: inline-block;
          position: relative;
      }

      canvas {
          background: black;
          display: block;
      }
      .lds-roller {
          display: none;
          width: 80px;
          height: 80px;

          position: absolute;
          top: 50%;
          left: 50%;
          transform: translate(-50%, -50%);
      }

      .is-loading .lds-roller {
          display: inline-block;
      }
      .lds-roller div {
          animation: lds-roller 1.2s cubic-bezier(0.5, 0, 0.5, 1) infinite;
          transform-origin: 40px 40px;
      }
      .lds-roller div:after {
          content: " ";
          display: block;
          position: absolute;
          width: 7px;
          height: 7px;
          border-radius: 50%;
          background: #fff;
          margin: -4px 0 0 -4px;
      }
      .lds-roller div:nth-child(1) {
          animation-delay: -0.036s;
      }
      .lds-roller div:nth-child(1):after {
          top: 63px;
          left: 63px;
      }
      .lds-roller div:nth-child(2) {
          animation-delay: -0.072s;
      }
      .lds-roller div:nth-child(2):after {
          top: 68px;
          left: 56px;
      }
      .lds-roller div:nth-child(3) {
          animation-delay: -0.108s;
      }
      .lds-roller div:nth-child(3):after {
          top: 71px;
          left: 48px;
      }
      .lds-roller div:nth-child(4) {
          animation-delay: -0.144s;
      }
      .lds-roller div:nth-child(4):after {
          top: 72px;
          left: 40px;
      }
      .lds-roller div:nth-child(5) {
          animation-delay: -0.18s;
      }
      .lds-roller div:nth-child(5):after {
          top: 71px;
          left: 32px;
      }
      .lds-roller div:nth-child(6) {
          animation-delay: -0.216s;
      }
      .lds-roller div:nth-child(6):after {
          top: 68px;
          left: 24px;
      }
      .lds-roller div:nth-child(7) {
          animation-delay: -0.252s;
      }
      .lds-roller div:nth-child(7):after {
          top: 63px;
          left: 17px;
      }
      .lds-roller div:nth-child(8) {
          animation-delay: -0.288s;
      }
      .lds-roller div:nth-child(8):after {
          top: 56px;
          left: 12px;
      }
      @keyframes lds-roller {
          0% {
              transform: rotate(0deg);
          }
          100% {
              transform: rotate(360deg);
          }
      }
    </style>

  </head>

  <body>
    <div class="wrapper is-loading">
      <canvas width="640" height="480"></canvas>
      <video hidden></video>
      <div class="lds-roller"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div>
    </div>

    <!-- Require the peer dependencies of face-landmarks-detection. -->
    <script src="https://unpkg.com/@tensorflow/tfjs-core@2.4.0/dist/tf-core.js"></script>
    <script src="https://unpkg.com/@tensorflow/tfjs-converter@2.4.0/dist/tf-converter.js"></script>

    <!-- You must explicitly require a TF.js backend if you're not using the tfjs union bundle. -->
    <script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@2.4.0/dist/tf-backend-webgl.js"></script>
    <!-- Alternatively you can use the WASM backend: <script src="https://unpkg.com/@tensorflow/tfjs-backend-wasm@2.4.0/dist/tf-backend-wasm.js"></script> -->

    <!-- Require face-landmarks-detection itself. -->
    <script src="https://unpkg.com/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>


    <script>
      var video = document.querySelector('video');
      video.setAttribute("playsinline", true);
      let model

      const canvas = document.querySelector('canvas');

      const ctx = canvas.getContext('2d');

      // let hasRendered = false

      const wrapper = document.querySelector('.wrapper')

      // ctx.fillRect(10, 10, 150, 100);

      const drawPoint = (x, y) => {
        ctx.fillStyle = 'white';
        ctx.beginPath()
        ctx.arc(x, y, 1, 0, 2 * Math.PI);
        ctx.fill()
      }

      const constraints = {
          video: { facingMode: "user" },
      }
      navigator.mediaDevices.getUserMedia(constraints)
               .then(function(stream) {
                   // Older browsers may not have srcObject
                   if ("srcObject" in video) {
                       video.srcObject = stream;
                   }

                   video.onloadedmetadata = function(e) {
                       video.play();
                   };
               })
               .catch(function(err) {
                   /* handle the error */
               });


      

      async function renderFace() {

          // Pass in a video stream (or an image, canvas, or 3D tensor) to obtain an
          // array of detected faces from the MediaPipe graph. If passing in a video
          // stream, a single prediction per frame will be returned.
          const predictions = await model.estimateFaces({
              input: video
          });

          // hasRendered = true
          wrapper.classList.remove('is-loading')

          ctx.clearRect(0, 0, canvas.width, canvas.height);

          if (predictions.length > 0) {
              /*
                `predictions` is an array of objects describing each detected face, for example:

                [
                {
                faceInViewConfidence: 1, // The probability of a face being present.
                boundingBox: { // The bounding box surrounding the face.
                topLeft: [232.28, 145.26],
                bottomRight: [449.75, 308.36],
                },
                mesh: [ // The 3D coordinates of each facial landmark.
                [92.07, 119.49, -17.54],
                [91.97, 102.52, -30.54],
                ...
                ],
                scaledMesh: [ // The 3D coordinates of each facial landmark, normalized.
                [322.32, 297.58, -17.54],
                [322.18, 263.95, -30.54]
                ],
                annotations: { // Semantic groupings of the `scaledMesh` coordinates.
                silhouette: [
                [326.19, 124.72, -3.82],
                [351.06, 126.30, -3.00],
                ...
                ],
                ...
                }
                }
                ]
              */

              for (let i = 0; i < predictions.length; i++) {
                  const keypoints = predictions[i].scaledMesh;

                  // drawPoint(keypoints[1][0], keypoints[1][1])
                  // drawPoint(keypoints[6][0], keypoints[6][1])

                  // Log facial keypoints.
                   for (let i = 0; i < keypoints.length; i++) {
                       const [x, y, z] = keypoints[i];
                       drawPoint(x, y)

                       // console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`);
                   }
              }
          }
          window.requestAnimationFrame(renderFace)
      }

      async function setup() {
        canvas.width = video.videoWidth
        canvas.height = video.videoHeight

          // Load the MediaPipe Facemesh package.
          model = await faceLandmarksDetection.load(
              faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);

          // canvas.addEventListener('click', renderFace)
          window.requestAnimationFrame(renderFace)
      }

      video.addEventListener('loadeddata', setup)
      
    </script>
  </body>
</html>
