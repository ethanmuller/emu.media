<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>moufs</title>

    <!-- this prevents a dorky zoomed-out page on mobile -->
    <meta name="viewport" content="width=device-width,initial-scale=1.0">

    <!-- this tells the browser which characters we're using -->
    <meta charset="UTF-8">

    <!-- this gets us nice fonts -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@300&display=swap" rel="stylesheet">


    <style>
      canvas {
          background: gray;
          display: block;
      }
    </style>

  </head>

  <body>
    <canvas width="640" height="480"></canvas>
    <video hidden></video>

    <!-- Require the peer dependencies of face-landmarks-detection. -->
    <script src="https://unpkg.com/@tensorflow/tfjs-core@2.4.0/dist/tf-core.js"></script>
    <script src="https://unpkg.com/@tensorflow/tfjs-converter@2.4.0/dist/tf-converter.js"></script>

    <!-- You must explicitly require a TF.js backend if you're not using the tfjs union bundle. -->
    <script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@2.4.0/dist/tf-backend-webgl.js"></script>
    <!-- Alternatively you can use the WASM backend: <script src="https://unpkg.com/@tensorflow/tfjs-backend-wasm@2.4.0/dist/tf-backend-wasm.js"></script> -->

    <!-- Require face-landmarks-detection itself. -->
    <script src="https://unpkg.com/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>


    <script>
      var video = document.querySelector('video');
      video.setAttribute("playsinline", true);
      let model

      const canvas = document.querySelector('canvas');

      const ctx = canvas.getContext('2d');

      ctx.fillStyle = 'white';
      // ctx.fillRect(10, 10, 150, 100);

      const drawPoint = (x, y) => {
        ctx.beginPath()
          ctx.arc(x, y, 1, 0, 2 * Math.PI);
        ctx.fill()
      }

      const constraints = {
          video: { facingMode: "user" },
      }
      navigator.mediaDevices.getUserMedia(constraints)
               .then(function(stream) {
                   // Older browsers may not have srcObject
                   if ("srcObject" in video) {
                       video.srcObject = stream;
                   }

                   video.onloadedmetadata = function(e) {
                       video.play();
                   };
               })
               .catch(function(err) {
                   /* handle the error */
               });


      

      async function main() {
          // Pass in a video stream (or an image, canvas, or 3D tensor) to obtain an
          // array of detected faces from the MediaPipe graph. If passing in a video
          // stream, a single prediction per frame will be returned.
          const predictions = await model.estimateFaces({
              input: video
          });

          ctx.clearRect(0, 0, canvas.width, canvas.height);

          if (predictions.length > 0) {
              /*
                `predictions` is an array of objects describing each detected face, for example:

                [
                {
                faceInViewConfidence: 1, // The probability of a face being present.
                boundingBox: { // The bounding box surrounding the face.
                topLeft: [232.28, 145.26],
                bottomRight: [449.75, 308.36],
                },
                mesh: [ // The 3D coordinates of each facial landmark.
                [92.07, 119.49, -17.54],
                [91.97, 102.52, -30.54],
                ...
                ],
                scaledMesh: [ // The 3D coordinates of each facial landmark, normalized.
                [322.32, 297.58, -17.54],
                [322.18, 263.95, -30.54]
                ],
                annotations: { // Semantic groupings of the `scaledMesh` coordinates.
                silhouette: [
                [326.19, 124.72, -3.82],
                [351.06, 126.30, -3.00],
                ...
                ],
                ...
                }
                }
                ]
              */

              for (let i = 0; i < predictions.length; i++) {
                  const keypoints = predictions[i].scaledMesh;

                  // drawPoint(keypoints[1][0], keypoints[1][1])
                  // drawPoint(keypoints[6][0], keypoints[6][1])

                  // Log facial keypoints.
                   for (let i = 0; i < keypoints.length; i++) {
                       const [x, y, z] = keypoints[i];
                       drawPoint(x, y)

                       // console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`);
                   }
              }
          }
      }

      async function setup() {
        canvas.width = video.videoWidth
        canvas.height = video.videoHeight

          // Load the MediaPipe Facemesh package.
          model = await faceLandmarksDetection.load(
              faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);

          canvas.addEventListener('click', main)
      }

      video.addEventListener('loadeddata', setup)
      
    </script>
  </body>
</html>
